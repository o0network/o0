{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Neural Network for Deep Q-learning\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "# Replay Buffer for experience replay\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(next_states),\n",
    "            np.array(dones),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# Agent implementing the DQN algorithm\n",
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size,\n",
    "        action_size,\n",
    "        alpha_min,\n",
    "        alpha_max,\n",
    "        r_min,\n",
    "        r_max,\n",
    "        num_alpha_steps=10,\n",
    "        num_r_steps=10,\n",
    "        discount_factor=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=0.995,\n",
    "        learning_rate=0.001,\n",
    "        batch_size=64,\n",
    "        update_target_freq=10,\n",
    "    ):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Define the discrete action space\n",
    "        self.alpha_range = np.linspace(alpha_min, alpha_max, num_alpha_steps)\n",
    "        self.r_range = np.linspace(r_min, r_max, num_r_steps)\n",
    "        self.alpha_steps = num_alpha_steps\n",
    "        self.r_steps = num_r_steps\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.gamma = discount_factor  # Discount factor\n",
    "        self.epsilon = epsilon_start  # Exploration rate\n",
    "        self.epsilon_min = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.update_target_freq = update_target_freq\n",
    "\n",
    "        # Policy and target networks\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(state_size, action_size).to(self.device)\n",
    "        self.target_net = DQN(state_size, action_size).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Replay buffer\n",
    "        self.memory = ReplayBuffer()\n",
    "\n",
    "        # Metrics\n",
    "        self.training_rewards = []\n",
    "        self.epsilon_history = []\n",
    "        self.avg_q_values = []\n",
    "        self.losses = []\n",
    "        self.steps = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # Exploration: choose a random action\n",
    "            return np.random.randint(0, self.action_size)\n",
    "        else:\n",
    "            # Exploitation: choose best action based on Q-values\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net(state_tensor)\n",
    "                self.avg_q_values.append(q_values.mean().item())\n",
    "                return q_values.argmax().item()\n",
    "\n",
    "    def action_to_params(self, action):\n",
    "        # Convert discrete action index to alpha and r values\n",
    "        alpha_idx = action // self.r_steps\n",
    "        r_idx = action % self.r_steps\n",
    "\n",
    "        alpha = self.alpha_range[alpha_idx]\n",
    "        r = self.r_range[r_idx]\n",
    "\n",
    "        return alpha, r\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Sample a batch of experiences\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(\n",
    "            self.batch_size\n",
    "        )\n",
    "\n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        # Compute Q(s_t, a) - the Q-values for the actions taken\n",
    "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "\n",
    "        # Compute Q(s_{t+1}, a) for all actions a\n",
    "        next_q_values = self.target_net(next_states).max(1)[0].detach()\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.smooth_l1_loss(q_values, expected_q_values.unsqueeze(1))\n",
    "        self.losses.append(loss.item())\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network periodically\n",
    "        self.steps += 1\n",
    "        if self.steps % self.update_target_freq == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        self.epsilon_history.append(self.epsilon)\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"policy_net\": self.policy_net.state_dict(),\n",
    "                \"target_net\": self.target_net.state_dict(),\n",
    "                \"optimizer\": self.optimizer.state_dict(),\n",
    "                \"epsilon\": self.epsilon,\n",
    "                \"steps\": self.steps,\n",
    "            },\n",
    "            path,\n",
    "        )\n",
    "\n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.policy_net.load_state_dict(checkpoint[\"policy_net\"])\n",
    "        self.target_net.load_state_dict(checkpoint[\"target_net\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        self.epsilon = checkpoint[\"epsilon\"]\n",
    "        self.steps = checkpoint[\"steps\"]\n",
    "\n",
    "\n",
    "# TokenomicsEnvironment simulates the blockchain environment\n",
    "class TokenomicsEnvironment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dex_data_path=None,  # Path to DEX trading data\n",
    "        initial_supply=1000000,\n",
    "        initial_alpha=100,\n",
    "        initial_r=0.3,  # 30% reserve ratio\n",
    "        episode_length=30,  # 30 days simulation\n",
    "        investor_weight=0.5,  # Weight for investor interests vs project interests\n",
    "        use_real_data=False,\n",
    "    ):\n",
    "        self.initial_supply = initial_supply\n",
    "        self.initial_alpha = initial_alpha\n",
    "        self.initial_r = initial_r\n",
    "        self.episode_length = episode_length\n",
    "        self.investor_weight = investor_weight\n",
    "        self.use_real_data = use_real_data\n",
    "        self.current_step = 0\n",
    "\n",
    "        # State variables\n",
    "        self.supply = initial_supply\n",
    "        self.price = self.calculate_price(initial_supply, initial_alpha, initial_r)\n",
    "        self.volume = 0\n",
    "        self.buys = 0\n",
    "        self.sells = 0\n",
    "        self.tvl = 0\n",
    "        self.qf_valuation = 0\n",
    "\n",
    "        # Metrics\n",
    "        self.price_stability = 100  # Higher is better\n",
    "        self.project_success_rate = 0\n",
    "        self.investor_return_rate = 0\n",
    "\n",
    "        # Load real DEX data if available\n",
    "        if use_real_data and dex_data_path:\n",
    "            self.dex_data = pd.read_csv(dex_data_path)\n",
    "        else:\n",
    "            # Generate synthetic data\n",
    "            self.dex_data = self.generate_synthetic_data()\n",
    "\n",
    "    def generate_synthetic_data(self):\n",
    "        # Create synthetic DEX data for simulation\n",
    "        data = []\n",
    "        base_price = self.price\n",
    "        base_volume = self.initial_supply * 0.01  # 1% daily volume\n",
    "\n",
    "        for day in range(90):  # Generate 90 days of data\n",
    "            # Add some randomness to price movements with a slight upward bias\n",
    "            price_change = np.random.normal(\n",
    "                0.002, 0.02\n",
    "            )  # Mean 0.2% daily growth, 2% std\n",
    "            price = base_price * (1 + price_change)\n",
    "\n",
    "            # Volume also varies with some correlation to absolute price change\n",
    "            volume_multiplier = (\n",
    "                1 + abs(price_change) * 5\n",
    "            )  # Higher volatility, higher volume\n",
    "            volume = base_volume * volume_multiplier\n",
    "\n",
    "            # Generate buy/sell ratio\n",
    "            if price_change > 0:\n",
    "                buy_ratio = 0.5 + np.random.random() * 0.3  # 50-80% buys on up days\n",
    "            else:\n",
    "                buy_ratio = 0.2 + np.random.random() * 0.3  # 20-50% buys on down days\n",
    "\n",
    "            buys = int(volume * buy_ratio)\n",
    "            sells = int(volume * (1 - buy_ratio))\n",
    "\n",
    "            # Generate QF valuations with some correlation to price trend\n",
    "            qf_trend = np.random.normal(\n",
    "                price_change * 2, 0.05\n",
    "            )  # More extreme than price\n",
    "            qf_valuation = max(0, self.initial_supply * self.price * (1 + qf_trend))\n",
    "\n",
    "            # Generate TVL\n",
    "            tvl = self.initial_supply * price * (0.4 + 0.2 * np.random.random())\n",
    "\n",
    "            data.append(\n",
    "                {\n",
    "                    \"day\": day,\n",
    "                    \"price\": price,\n",
    "                    \"volume\": volume,\n",
    "                    \"buys\": buys,\n",
    "                    \"sells\": sells,\n",
    "                    \"tvl\": tvl,\n",
    "                    \"qf_valuation\": qf_valuation,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            base_price = price  # Update base price for next iteration\n",
    "\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    def calculate_price(self, supply, alpha, r):\n",
    "        # ABC formula: P(s) = alpha * s^beta, where beta = 1/(1-r)\n",
    "        if supply == 0:\n",
    "            return alpha\n",
    "\n",
    "        beta = 1 / (1 - r)\n",
    "        return alpha * (supply**beta)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.supply = self.initial_supply\n",
    "        self.price = self.calculate_price(\n",
    "            self.initial_supply, self.initial_alpha, self.initial_r\n",
    "        )\n",
    "\n",
    "        # Reset metrics\n",
    "        self.price_stability = 100\n",
    "        self.project_success_rate = 0\n",
    "        self.investor_return_rate = 0\n",
    "\n",
    "        # Get initial state\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        # Get data for current day\n",
    "        if self.use_real_data:\n",
    "            day_data = self.dex_data.iloc[self.current_step % len(self.dex_data)]\n",
    "        else:\n",
    "            day_data = self.dex_data.iloc[self.current_step % len(self.dex_data)]\n",
    "\n",
    "        # Normalize state values for neural network\n",
    "        state = np.array(\n",
    "            [\n",
    "                self.price / 1000,  # Normalized price\n",
    "                self.supply / self.initial_supply,  # Relative supply\n",
    "                day_data[\"volume\"] / self.initial_supply,  # Volume as % of supply\n",
    "                day_data[\"buys\"]\n",
    "                / (day_data[\"buys\"] + day_data[\"sells\"] + 1e-10),  # Buy ratio\n",
    "                day_data[\"tvl\"] / (self.supply * self.price + 1e-10),  # TVL ratio\n",
    "                day_data[\"qf_valuation\"]\n",
    "                / (self.supply * self.price + 1e-10),  # QF valuation ratio\n",
    "                self.price_stability / 100,  # Normalized stability\n",
    "                self.project_success_rate / 100,  # Normalized project success\n",
    "                self.investor_return_rate / 100,  # Normalized investor return\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return state\n",
    "\n",
    "    def step(self, action_params):\n",
    "        # Unpack action parameters\n",
    "        alpha, r = action_params\n",
    "\n",
    "        # Get data for current day\n",
    "        if self.use_real_data:\n",
    "            day_data = self.dex_data.iloc[self.current_step % len(self.dex_data)]\n",
    "        else:\n",
    "            day_data = self.dex_data.iloc[self.current_step % len(self.dex_data)]\n",
    "\n",
    "        # Store old price for comparison\n",
    "        old_price = self.price\n",
    "\n",
    "        # Calculate new price with new parameters\n",
    "        new_price = self.calculate_price(self.supply, alpha, r)\n",
    "\n",
    "        # Update price\n",
    "        price_change_pct = abs(new_price - old_price) / old_price\n",
    "\n",
    "        # Calculate stability - lower price volatility is better\n",
    "        new_stability = 100 * (\n",
    "            1 - min(1, price_change_pct * 5)\n",
    "        )  # Penalize large price changes\n",
    "        self.price_stability = (\n",
    "            0.7 * self.price_stability + 0.3 * new_stability\n",
    "        )  # Weighted average\n",
    "\n",
    "        # Simulate token demand based on price change\n",
    "        if new_price > old_price:\n",
    "            # Price increase: fewer buys (demand elasticity)\n",
    "            buy_adjustment = max(0.5, 1 - price_change_pct * 3)\n",
    "            adjusted_buys = day_data[\"buys\"] * buy_adjustment\n",
    "\n",
    "            # More sells due to profit-taking\n",
    "            sell_adjustment = min(2.0, 1 + price_change_pct * 5)\n",
    "            adjusted_sells = day_data[\"sells\"] * sell_adjustment\n",
    "        else:\n",
    "            # Price decrease: more buys (buying the dip)\n",
    "            buy_adjustment = min(2.0, 1 + price_change_pct * 2)\n",
    "            adjusted_buys = day_data[\"buys\"] * buy_adjustment\n",
    "\n",
    "            # More sells due to fear\n",
    "            sell_adjustment = min(2.5, 1 + price_change_pct * 8)\n",
    "            adjusted_sells = day_data[\"sells\"] * sell_adjustment\n",
    "\n",
    "        # Update supply based on net buys/sells\n",
    "        net_tokens = adjusted_buys - adjusted_sells\n",
    "        self.supply += net_tokens\n",
    "        self.supply = max(\n",
    "            self.supply, self.initial_supply * 0.1\n",
    "        )  # Don't let supply go too low\n",
    "\n",
    "        # Update actual price based on new supply\n",
    "        self.price = self.calculate_price(self.supply, alpha, r)\n",
    "\n",
    "        # Update project success rate based on QF valuation and price\n",
    "        qf_price_ratio = day_data[\"qf_valuation\"] / (self.supply * self.price + 1e-10)\n",
    "        self.project_success_rate = 0.8 * self.project_success_rate + 0.2 * min(\n",
    "            100, qf_price_ratio * 50\n",
    "        )\n",
    "\n",
    "        # Update investor return rate based on price change\n",
    "        if self.current_step > 0:\n",
    "            price_return = (self.price / old_price - 1) * 100  # Percentage return\n",
    "            self.investor_return_rate = 0.8 * self.investor_return_rate + 0.2 * min(\n",
    "                100, max(0, 50 + price_return)\n",
    "            )\n",
    "\n",
    "        # Calculate reward\n",
    "        price_stability_reward = self.price_stability / 25  # Scale from 0-4\n",
    "        project_reward = self.project_success_rate / 25  # Scale from 0-4\n",
    "        investor_reward = self.investor_return_rate / 25  # Scale from 0-4\n",
    "\n",
    "        # Balance investor and project interests\n",
    "        reward = (\n",
    "            price_stability_reward * 0.2  # 20% for stability\n",
    "            + investor_reward\n",
    "            * self.investor_weight\n",
    "            * 0.8  # 80% split between investor and project\n",
    "            + project_reward * (1 - self.investor_weight) * 0.8\n",
    "        )\n",
    "\n",
    "        # Increment step\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.episode_length\n",
    "\n",
    "        return (\n",
    "            self.get_state(),\n",
    "            reward,\n",
    "            done,\n",
    "            {\n",
    "                \"price\": self.price,\n",
    "                \"supply\": self.supply,\n",
    "                \"price_stability\": self.price_stability,\n",
    "                \"project_success_rate\": self.project_success_rate,\n",
    "                \"investor_return_rate\": self.investor_return_rate,\n",
    "            },\n",
    "        )\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_dqn(agent, env, num_episodes=1000, max_steps=30):\n",
    "    rewards_history = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Select action\n",
    "            action = agent.select_action(state)\n",
    "            action_params = agent.action_to_params(action)\n",
    "\n",
    "            # Take action in environment\n",
    "            next_state, reward, done, info = env.step(action_params)\n",
    "\n",
    "            # Store transition in replay buffer\n",
    "            agent.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "            # Update network\n",
    "            agent.update()\n",
    "\n",
    "            # Update state and accumulate reward\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Record metrics\n",
    "        agent.training_rewards.append(episode_reward)\n",
    "\n",
    "        # Print progress\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(agent.training_rewards[-10:])\n",
    "            avg_loss = np.mean(agent.losses[-100:]) if agent.losses else 0\n",
    "            avg_q = np.mean(agent.avg_q_values[-100:]) if agent.avg_q_values else 0\n",
    "            print(\n",
    "                f\"Episode {episode+1}/{num_episodes}, Avg Reward: {avg_reward:.2f}, \"\n",
    "                f\"Epsilon: {agent.epsilon:.2f}, Avg Loss: {avg_loss:.4f}, Avg Q: {avg_q:.2f}\"\n",
    "            )\n",
    "\n",
    "            # Save model checkpoint\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                agent.save(f\"dqn_model_episode_{episode+1}.pt\")\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "# Comparison function between ABC+QF with and without RL\n",
    "def compare_with_without_rl(dex_data_path=None, num_episodes=1):\n",
    "    # Environment setup\n",
    "    env_with_rl = TokenomicsEnvironment(\n",
    "        dex_data_path=dex_data_path, use_real_data=dex_data_path is not None\n",
    "    )\n",
    "    env_without_rl = TokenomicsEnvironment(\n",
    "        dex_data_path=dex_data_path, use_real_data=dex_data_path is not None\n",
    "    )\n",
    "\n",
    "    # RL agent (already trained)\n",
    "    state_size = 9  # Number of state variables\n",
    "    action_size = 100  # 10 alpha steps x 10 r steps\n",
    "    agent = DQNAgent(\n",
    "        state_size=state_size,\n",
    "        action_size=action_size,\n",
    "        alpha_min=50,\n",
    "        alpha_max=500,\n",
    "        r_min=0.1,\n",
    "        r_max=0.9,\n",
    "        num_alpha_steps=10,\n",
    "        num_r_steps=10,\n",
    "    )\n",
    "\n",
    "    # Load trained model\n",
    "    try:\n",
    "        agent.load(\"dqn_model_final.pt\")\n",
    "    except:\n",
    "        print(\"No trained model found. Using untrained agent.\")\n",
    "\n",
    "    # Results storage\n",
    "    results_with_rl = {\n",
    "        \"price\": [],\n",
    "        \"stability\": [],\n",
    "        \"project_success\": [],\n",
    "        \"investor_return\": [],\n",
    "    }\n",
    "\n",
    "    results_without_rl = {\n",
    "        \"price\": [],\n",
    "        \"stability\": [],\n",
    "        \"project_success\": [],\n",
    "        \"investor_return\": [],\n",
    "    }\n",
    "\n",
    "    # Simulation with RL\n",
    "    state = env_with_rl.reset()\n",
    "    for step in range(env_with_rl.episode_length):\n",
    "        action = agent.select_action(state)\n",
    "        action_params = agent.action_to_params(action)\n",
    "\n",
    "        state, _, done, info = env_with_rl.step(action_params)\n",
    "\n",
    "        # Store results\n",
    "        results_with_rl[\"price\"].append(info[\"price\"])\n",
    "        results_with_rl[\"stability\"].append(info[\"price_stability\"])\n",
    "        results_with_rl[\"project_success\"].append(info[\"project_success_rate\"])\n",
    "        results_with_rl[\"investor_return\"].append(info[\"investor_return_rate\"])\n",
    "\n",
    "    # Simulation without RL (fixed parameters)\n",
    "    fixed_alpha = env_without_rl.initial_alpha\n",
    "    fixed_r = env_without_rl.initial_r\n",
    "\n",
    "    state = env_without_rl.reset()\n",
    "    for step in range(env_without_rl.episode_length):\n",
    "        state, _, done, info = env_without_rl.step((fixed_alpha, fixed_r))\n",
    "\n",
    "        # Store results\n",
    "        results_without_rl[\"price\"].append(info[\"price\"])\n",
    "        results_without_rl[\"stability\"].append(info[\"price_stability\"])\n",
    "        results_without_rl[\"project_success\"].append(info[\"project_success_rate\"])\n",
    "        results_without_rl[\"investor_return\"].append(info[\"investor_return_rate\"])\n",
    "\n",
    "    # Plot comparison\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Plot price\n",
    "    axs[0, 0].plot(results_with_rl[\"price\"], \"b-\", label=\"With RL\")\n",
    "    axs[0, 0].plot(results_without_rl[\"price\"], \"r--\", label=\"Without RL\")\n",
    "    axs[0, 0].set_title(\"Token Price\")\n",
    "    axs[0, 0].set_xlabel(\"Days\")\n",
    "    axs[0, 0].set_ylabel(\"Price\")\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    # Plot stability\n",
    "    axs[0, 1].plot(results_with_rl[\"stability\"], \"b-\", label=\"With RL\")\n",
    "    axs[0, 1].plot(results_without_rl[\"stability\"], \"r--\", label=\"Without RL\")\n",
    "    axs[0, 1].set_title(\"Price Stability\")\n",
    "    axs[0, 1].set_xlabel(\"Days\")\n",
    "    axs[0, 1].set_ylabel(\"Stability Score\")\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    # Plot project success\n",
    "    axs[1, 0].plot(results_with_rl[\"project_success\"], \"b-\", label=\"With RL\")\n",
    "    axs[1, 0].plot(results_without_rl[\"project_success\"], \"r--\", label=\"Without RL\")\n",
    "    axs[1, 0].set_title(\"Project Success Rate\")\n",
    "    axs[1, 0].set_xlabel(\"Days\")\n",
    "    axs[1, 0].set_ylabel(\"Success Rate\")\n",
    "    axs[1, 0].legend()\n",
    "\n",
    "    # Plot investor return\n",
    "    axs[1, 1].plot(results_with_rl[\"investor_return\"], \"b-\", label=\"With RL\")\n",
    "    axs[1, 1].plot(results_without_rl[\"investor_return\"], \"r--\", label=\"Without RL\")\n",
    "    axs[1, 1].set_title(\"Investor Return Rate\")\n",
    "    axs[1, 1].set_xlabel(\"Days\")\n",
    "    axs[1, 1].set_ylabel(\"Return Rate\")\n",
    "    axs[1, 1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"rl_comparison.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Return comparison metrics\n",
    "    return {\n",
    "        \"with_rl\": {\n",
    "            \"avg_price\": np.mean(results_with_rl[\"price\"]),\n",
    "            \"avg_stability\": np.mean(results_with_rl[\"stability\"]),\n",
    "            \"avg_project_success\": np.mean(results_with_rl[\"project_success\"]),\n",
    "            \"avg_investor_return\": np.mean(results_with_rl[\"investor_return\"]),\n",
    "        },\n",
    "        \"without_rl\": {\n",
    "            \"avg_price\": np.mean(results_without_rl[\"price\"]),\n",
    "            \"avg_stability\": np.mean(results_without_rl[\"stability\"]),\n",
    "            \"avg_project_success\": np.mean(results_without_rl[\"project_success\"]),\n",
    "            \"avg_investor_return\": np.mean(results_without_rl[\"investor_return\"]),\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    random.seed(42)\n",
    "\n",
    "    # Create environment\n",
    "    env = TokenomicsEnvironment()\n",
    "\n",
    "    # Define state and action sizes\n",
    "    state_size = 9  # Number of state variables\n",
    "    action_size = 100  # 10 alpha steps x 10 r steps\n",
    "\n",
    "    # Create agent\n",
    "    agent = DQNAgent(\n",
    "        state_size=state_size,\n",
    "        action_size=action_size,\n",
    "        alpha_min=50,\n",
    "        alpha_max=500,\n",
    "        r_min=0.1,\n",
    "        r_max=0.9,\n",
    "        num_alpha_steps=10,\n",
    "        num_r_steps=10,\n",
    "    )\n",
    "\n",
    "    # Train agent\n",
    "    print(\"Starting training...\")\n",
    "    trained_agent = train_dqn(agent, env, num_episodes=500)\n",
    "\n",
    "    # Save final model\n",
    "    trained_agent.save(\"dqn_model_final.pt\")\n",
    "\n",
    "    # Compare with and without RL\n",
    "    print(\"Comparing performance with and without RL...\")\n",
    "    comparison = compare_with_without_rl()\n",
    "\n",
    "    print(\"\\nAverage metrics with RL:\")\n",
    "    for key, value in comparison[\"with_rl\"].items():\n",
    "        print(f\"{key}: {value:.2f}\")\n",
    "\n",
    "    print(\"\\nAverage metrics without RL:\")\n",
    "    for key, value in comparison[\"without_rl\"].items():\n",
    "        print(f\"{key}: {value:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
